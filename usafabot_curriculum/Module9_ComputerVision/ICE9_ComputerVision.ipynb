{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edfba5c0",
   "metadata": {},
   "source": [
    "# Module 9: Computer Vision\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f8bc27",
   "metadata": {},
   "source": [
    "## Image Basics\n",
    "When we talk about the sizes of images, we generally talk about them in terms of the number of pixels the image possesses in the x(horizontal) or y(vertical) direction.  If the image is a color image, we also need to concern ourselves with the depth of the image as well.  Normally, each individual pixel is represented by the “color” or the “intensity” of light that appears in a given place in our image.\n",
    "\n",
    "If we think of an image as a grid, each square in the grid contains a single pixel.\n",
    "\n",
    "Most pixels are represented in two ways: grayscale and color. In a grayscale image, each pixel has a value between 0 and 255, where zero is corresponds to “black” and 255 being “white”. The values in between 0 and 255 are varying shades of gray, where values closer to 0 are darker and values closer 255 are lighter:\n",
    "\n",
    "<img src=\"Figures/Grayscale.JPG\" width=\"400\" height=\"195.89\">\n",
    "\n",
    "The grayscale gradient image in the figure above demonstrates darker pixels on the left-hand side and progressively lighter pixels on the right-hand side.\n",
    "\n",
    "Color pixels, however, are normally represented in the RGB color space (this is where the term color-depth comes from)— one value for the Red component, one for Green, and one for Blue, leading to a total of 3 values per pixel:\n",
    "\n",
    "<img src=\"Figures/RGB.JPG\" width=\"300\" height=\"195.89\">\n",
    "\n",
    "Other color spaces exist, and ordering of the colors may differ as well, but let’s start with the common RGB system.  If we say the image is a 24-bit image, each of the three Red, Green, and Blue colors are represented by an integer in the range 0 to 255 (8-bits), which indicates how “much” of the color there is. Given that the pixel value only needs to be in the range [0, 255] we normally use an 8-bit unsigned integer to represent each color intensity.  We then combine these values into a RGB tuple in the form (red, green, blue) . This tuple represents our color.  For example:\n",
    "\n",
    "- To construct a white color, we would fill each of the red, green, and blue buckets completely up, like this: (255, 255, 255) — since white is the presence of all color.\n",
    "- Then, to create a black color, we would empty each of the buckets out: (0, 0, 0) — since black is the absence of color.\n",
    "- To create a pure red color, we would fill up the red bucket (and only the red bucket) up completely: (255, 0, 0) .\n",
    "- etc\n",
    "\n",
    "Take a look at the following image to make this concept more clear:\n",
    "\n",
    "<figure>\n",
    "<img src=\"Figures/RGB_Tuple.JPG\" width=\"400\" height=\"195.89\" alt=\"RGB\" >\n",
    "</figure>\n",
    "\n",
    "For your reference, here are some common colors represented as RGB tuples:\n",
    "\n",
    "- Black:  (0, 0, 0)\n",
    "- White:  (255, 255, 255)\n",
    "- Red:  (255, 0, 0)\n",
    "- Green:  (0, 255, 0)\n",
    "- Blue:  (0, 0, 255)\n",
    "- Aqua:  (0, 255, 255)\n",
    "- Fuchsia:  (255, 0, 255)\n",
    "- Maroon:  (128, 0, 0)\n",
    "- Navy:  (0, 0, 128)\n",
    "- Olive:  (128, 128, 0)\n",
    "- Purple:  (128, 0, 128)\n",
    "- Teal:  (0, 128, 128)\n",
    "- Yellow:  (255, 255, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd28861",
   "metadata": {},
   "source": [
    "## Coding with OpenCV-Python\n",
    "It is time to build our first bit of code working with OpenCV.  Just like ROS, OpenCV is well supported by both Python and C++.  For simplicity, we will use Python throughout this course.  However, continue to recognize that if speed and efficiency become important, switching to a more robust language like C++ may become necessary.  To make use of OpenCV with Python, we need to import cv2.  The code below will simply load in the RGB figure above and print out the pixel values in each of the 4-quadrants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89efcefa",
   "metadata": {},
   "source": [
    "First we need to import the OpenCV Python library, `cv2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad16827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79091fcc",
   "metadata": {},
   "source": [
    "Then we can load the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8aacf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"RGB_Tuple.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba72f3f",
   "metadata": {},
   "source": [
    "The `shape` characteristic of the image returns a tuple of the number of rows, columns, and channels (if the image is color):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc18c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"width: %d pixels\" %(image.shape[1]))\n",
    "print(\"height: %d pixels\" % (image.shape[0]))\n",
    "print(\"color channels: %d\" % (image.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e86cd0",
   "metadata": {},
   "source": [
    "You an also access specific pixels within the image (the `image` variable is really just an array of pixel values) by the row and column coordinates. Each pixel values is an array of Blue, Green, and Red values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265672bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the BGR values of a pixel in the upper left of the image\n",
    "print(image[10, 10, :])\n",
    "\n",
    "# print the red value of a pixel in the bottom left of the image\n",
    "print(image[700, 100, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee50526",
   "metadata": {},
   "source": [
    "Fill in the code to do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e490440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print BGR values of a pixel in the upper right of the image\n",
    "print(image[ , , :])\n",
    "\n",
    "# TODO: print BGR values of a pixel in the lower left of the image\n",
    "print(image[ , , :])\n",
    "\n",
    "# TODO: print blue value of a pixel in the lower right of the image\n",
    "print(image[ , , ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024de5ad",
   "metadata": {},
   "source": [
    "We can display the image as well.\n",
    "> ⚠️ **WARNING:** To exit the image just press any key. **DO NOT** press the 'X' in the corner. If you do press the 'X' (smh) you will have to Restart & Clear the Kernel: in the Jupyter Notebook at the top menu bar select \"Kernel\" and \"Restart & Clear Output\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e52512",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Loaded image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() # close the image window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6013b52",
   "metadata": {},
   "source": [
    "When executing the code above, there were two minor surprises. What do you think they were? Now lets take a look at additional functionality embedded within OpenCV.\n",
    "\n",
    "Convert image to RGB and print the same pixel values. Remember that the image is already loaded within the `image` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert image to RGB\n",
    "\n",
    "\n",
    "# TODO: print the RGB values of a pixel in the upper left of the image\n",
    "\n",
    "\n",
    "# TODO: print the red value of a pixel in the bottom left of the image\n",
    "\n",
    "\n",
    "# TODO: print RGB values of a pixel in the upper right of the image\n",
    "\n",
    "\n",
    "# TODO: print RGB values of a pixel in the lower left of the image\n",
    "\n",
    "\n",
    "# TODO: print blue value of a pixel in the lower right of the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49f6a7",
   "metadata": {},
   "source": [
    "Modify the code to convert to grayscale and print the same pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d772a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert image to Grayscale\n",
    "\n",
    "\n",
    "# TODO: print the Grayscale values of a pixel in the upper left of the image\n",
    "\n",
    "\n",
    "# TODO: print Grayscale values of a pixel in the upper right of the image\n",
    "\n",
    "\n",
    "# TODO: print Grayscale values of a pixel in the lower left of the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2956ed9",
   "metadata": {},
   "source": [
    "### Summary\n",
    "These examples barely scratch the surface of what is possible with OpenCV. In the upcoming lessons we will learn a few more ways to manipulate images, but if you want to learn more you can either explore the [OpenCV-Python Source Documentation](https://docs.opencv.org/3.4/index.html) or the [OpenCV-Python Turtorial](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html).\n",
    "\n",
    "### Assignment\n",
    "Scan the article on the [Histogram of Oriented Gradients (HOG)](https://arxiv.org/pdf/1406.2419.pdf) feature discriptor and be prepared to discuss.  I don't need you to understand the math, but you should be able to understand the advantages of the technique.\n",
    "\n",
    "### Cleanup\n",
    "In the Jupyter Notebook at the top menu bar select \"Kernel\" and \"Restart & Clear Output\". Shutdown the notebook server by typing `ctrl+c` within the terminal you ran `jupyter-notebook` in. Select 'y'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c2b46",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "The objective of this portion of the lesson is for you to start the process of learning how to create custom object detectors in an image.  There are many techniques, but the one technique I am interested in applying first is what is known as Histogram of Oriented Gradients.  Before we can dig into the technique, we should first understand a bit about image gradients and contours.\n",
    "\n",
    "By the end of today's lesson you’ll be able to: \n",
    "- Define what an image gradient is \n",
    "- Compute changes in direction of an input image \n",
    "- Define both gradient magnitude and gradient orientation \n",
    "- Use OpenCV to approximate image gradients \n",
    "\n",
    "The image gradient is one of the fundamental building blocks in computer vision image processing.   \n",
    "\n",
    "We use gradients for detecting edges in images, which allows us to find contours and outlines of objects in images.  We use them as inputs for quantifying images through feature extraction — in fact, highly successful and well-known image descriptors such as Histogram of Oriented Gradients and SIFT are built upon image gradient representations. Gradient images are even used to construct saliency maps, which highlight the subjects of an image.  We use gradients all the time in computer vision and image processing.  I would go as far as to say they are one of the most important building blocks you’ll learn about in this module.  While they are not often discussed in detail since other more powerful and interesting methods build on top of them, we are going to take the time and discuss them in detail. \n",
    "\n",
    "As I mentioned in the introduction, image gradients are used as the basic building blocks in many computer vision and image processing applications.  However, the main application of image gradients lies within edge detection.  As the name suggests, edge detection is the process of finding edges in an image, which reveals structural information regarding the objects in an image.  Edges could therefore correspond to: \n",
    "\n",
    "- Boundaries of an object in an image. \n",
    "- Boundaries of shadowing or lighting conditions in an image. \n",
    "- Boundaries of “parts” within an object. \n",
    "\n",
    "As we mentioned in the previous portion of the lab, we will often work with grayscale images, because of the massive reduction in images.  OpenCV will convert to grayscale using the following conversion formula:\n",
    "\n",
    "$Y = 0.299 R + 0.587 G + 0.114 B$\n",
    "\n",
    "Let's see if that matches our expectations in the figure below:\n",
    "\n",
    "<img src=\"Figures/RGB_Gray.png\" width=\"400\" height=\"195.89\">\n",
    "\n",
    "The below figure is an image of edges being detected simply by looking for the contours in an image:\n",
    "\n",
    "<img src=\"Figures/EdgeDet.png\" width=\"400\" height=\"195.89\">\n",
    "\n",
    "As you can see, all of the edges (or changes in contrast are clearly identified), but how did we do it?  Lets look at the math below, and then we will look at how simple the code is by taking advantage of OpenCV.\n",
    "\n",
    "Formally, an image gradient is defined as a directional change in image intensity.  Or put more simply, at each pixel of the input (grayscale) image, a gradient measures the change in pixel intensity in a given direction. By estimating the direction or orientation along with the magnitude (i.e. how strong the change in direction is), we are able to detect regions of an image that look like edges.\n",
    "\n",
    "Lets look at a blown up version of a basic pixel map.  Our goal here is to establish the basic framework for how we will eventually compute the gradient:\n",
    "\n",
    "<img src=\"Figures/PixelCont.png\" width=\"300\" height=\"195.89\">\n",
    "\n",
    "In the image above we essentially wish to examine the \\(3 \\times 3\\) neighborhood surrounding the central pixel. Our x values run from left to right, and our y values from top to bottom. In order to compute any changes in direction we’ll need the north, south, east, and west pixels, which are marked on the above figure.\n",
    "\n",
    "If we denote our input image as *I*, then we define the north, south, east, and west pixels using the following notation:\n",
    "\n",
    "- North: $I(x, y - 1)$\n",
    "- South: $I(x, y + 1)$\n",
    "- East: $I(x + 1, y)$\n",
    "- West: $I(x - 1, y)$\n",
    "\n",
    "Again, these four values are critical in computing the changes in image intensity in both the x and y direction.\n",
    "\n",
    "To demonstrate this, let’s compute the vertical change or the y-change by taking the difference between the south and north pixels:\n",
    "\n",
    "$G_{y} = I(x, y + 1) - I(x, y - 1)$\n",
    "\n",
    "Similarly, we can compute the horizontal change or the x-change by taking the difference between the east and west pixels:\n",
    "\n",
    "$G_{x} = I(x + 1, y) - I(x - 1, y)$\n",
    "\n",
    "Awesome — so now we have $G_{x}$ and $G_{y}$, which represent the change in image intensity for the central pixel in both the x and y direction.  Lets look at a relatively intuitive example at first without all the math.\n",
    "\n",
    "<img src=\"Figures/GradientEx.png\" width=\"600\" height=\"195.89\">\n",
    "\n",
    "On the left we have a $3 \\times 3$ region of an image where the top half of the image is white and the bottom half of the image is black. The gradient orientation is thus equal to $\\theta=-90^{\\circ}$\n",
    "\n",
    "And on the right we have another $3 \\times 3$ neighborhood of an image, where the upper triangular region is white and the lower triangular region is black. Here we can see the change in direction is equal to $\\theta=-45^{\\circ}$.  While these two examples are both relatively easy to understand, lets use our knowledge of the Pythagorean theorem to actually compute the magnitude and orientation of the gradient with actual values now.\n",
    "\n",
    "<img src=\"Figures/GradTrig.png\" width=\"300\" height=\"195.89\">\n",
    "\n",
    "Inspecting the triangle in the above figure you can see that the gradient magnitude G is the hypotenuse of the triangle. Therefore, all we need to do is apply the Pythagorean theorem and we’ll end up with the gradient magnitude:\n",
    "\n",
    "$G = \\sqrt{G_{x}^{2} + G_{y}^{2}}$\n",
    "\n",
    "The gradient orientation can then be given as the ratio of $G_{y}$ to $G_{x}$. We can use the $tan^{-1}$ to compute the gradient orientation,\n",
    "\n",
    "$\\theta = tan^{-1}(\\frac{G_{y}}{G_{x}}) \\times (\\frac{180}{\\pi})$\n",
    "\n",
    "We converted to degrees by multiplying by the ratio of $180/\\pi$.  Lets now add pixel intensity values and put this to the test.\n",
    "\n",
    "<img src=\"Figures/GradEx2.png\" width=\"600\" height=\"195.89\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f68b2b",
   "metadata": {},
   "source": [
    "In the above image we have an image where the upper-third is white and the bottom two-thirds is black. Using the equations for $G_{x}$ and $G_{y}$, we arrive at:\n",
    "\n",
    "$G_{x} = $\n",
    "\n",
    "and\n",
    "\n",
    "$G_{y} = $\n",
    "\n",
    "Plugging these values into our gradient magnitude equation we get:\n",
    "\n",
    "$G = $\n",
    "\n",
    "As for our gradient orientation:\n",
    "\n",
    "$\\theta = $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be8916",
   "metadata": {},
   "source": [
    "Now you try with the following example:\n",
    "\n",
    "<img src=\"Figures/GradEx3.png\" width=\"600\" height=\"195.89\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74deabb",
   "metadata": {},
   "source": [
    "$G_{x} = $\n",
    "\n",
    "and\n",
    "\n",
    "$G_{y} = $\n",
    "\n",
    "Plugging these values into our gradient magnitude equation we get:\n",
    "\n",
    "$G = $\n",
    "\n",
    "As for our gradient orientation:\n",
    "\n",
    "$\\theta = $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4f01a",
   "metadata": {},
   "source": [
    "Now that you know how to compute both the orientation and the magnitude of the gradients, you essentially have the most basic building block established for computing the necessary information for HOG w/ SVM.  Additionally, you can use the following code to compute very effective contours in images.  Go ahead and put this into your own client to test it out and explore.\n",
    "\n",
    "Fortunately, in practice we don't need to do any of the math above.  Instead we can use what is known as the Sobel Kernel to compute the values for $G_{x}$ and $G_{y}$.  OpenCV and numpy have functionality built in that allow us to do all of this very quickly.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017ffc8",
   "metadata": {},
   "source": [
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#load the image\n",
    "image=cv2.imread(\"RGB_Tuple.JPG\")\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Show the original image along with the grayscale image\n",
    "cv2.imshow(\"Original image\", image)\n",
    "cv2.imshow(\"Grayscale Image\", gray)\n",
    "\n",
    "# Lets now compute the gradients along the X and Y axis, respectively\n",
    "gX = cv2.Sobel(gray,cv2.CV_64F,1,0)\n",
    "gY = cv2.Sobel(gray,cv2.CV_64F,0,1)\n",
    "\n",
    "# the `gX` and `gY` images are now of the floating point data type,\n",
    "# so we need to take care to convert them back to an unsigned 8-bit\n",
    "# integer representation so other OpenCV functions can utilize them\n",
    "gX = cv2.convertScaleAbs(gX)\n",
    "gY = cv2.convertScaleAbs(gY)\n",
    "\n",
    "# combine the sobel X and Y representations into a single image\n",
    "sobelCombined = cv2.addWeighted(gX, 0.5, gY, 0.5, 0)\n",
    "cv2.imshow(\"Gradient Image\", sobelCombined)\n",
    "cv2.waitKey(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e3a68",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Gradients are one important tool used in object detection. Next lesson we will learn how to apply gradients using the Histogram of Oriented Gradients to train an object detector.\n",
    "\n",
    "### Assignment\n",
    "Watch the following video on [Histogram of Oriented Gradients](https://youtube.com/watch?v=4ESLTAd3IOM).\n",
    "\n",
    "### Cleanup\n",
    "In the Jupyter Notebook at the top menu bar select \"Kernel\" and \"Restart & Clear Output\". Shutdown the notebook server by typing `ctrl+c` within the terminal you ran `jupyter-notebook` in. Select 'y'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5b813",
   "metadata": {},
   "source": [
    "## Histogram of Gradients (HOG) Features\n",
    "\n",
    "The objective of this portion of the lesson is to demonstrate the functionality of the HOG with SVM algorithm for object detection.  By this point, we should all be well aware of what a histogram is.  The application of the histogram for the HOG feature extraction is to further simplify the tested image to enable our computer to rapidly and accurately identify the presence of an object within the image.  \n",
    "Instead of using each individual gradient direction of each individual pixel of an image, we group the pixels into small cells. For each cell, we compute all the gradient directions and group them into a number of orientation bins. We sum up the gradient magnitude in each sample. So stronger gradients contribute more weight to their bins, and effects of small random orientations due to noise is reduced. Doing this for all cells gives us a representation of the structure of the image. The HOG features keep the representation of an object distinct but also allow for some variations in shape.  For example, lets consider an object detector for a car, see the below figure.\n",
    "\n",
    "<img src=\"Figures/HOG_Features.JPG\" width=\"600\" height=\"195.89\">\n",
    "\n",
    "Comparing each individual pixel of this training image with another test image would not only be time consuming, but it would also be highly subject to noise.  As previously mentioned, the HOG feature will consider a block of pixels.  The size of this block is variable and will naturally impact both accuracy and speed of execution for the algorithm.  Once the block size is determined, the gradient for each pixel within the block is computed.  Once the gradients are computed for a block, the entire cell can then be represented by this histogram.  Not only does this reduce the amount of data to compare with a test image, but it also reduces the impacts of noise in the image and measurements.  \n",
    "\n",
    "<img src=\"Figures/HOG_Histogram.JPG\" width=\"600\" height=\"195.89\">\n",
    "\n",
    "Now that we have an understanding of the HOG features, lets use tools embedded within OpenCV and Dlib to build our first detector for a stop sign.  But first we need to download a pre-created repository of test and training data.  Remember, we won't use our training data to test the effectiveness of the algorithm.  Of course the algorithm will work effectively on the training data.  Our hope is that we can create a large enough sampling of test data that we can have a highly effective detector that is robust against new images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee3c54",
   "metadata": {},
   "source": [
    "Download the example demo into the `my_scripts` folder you created earlier in the semester. It should be located under `~/master_ws/src/usafabot/usafabot_curriculum/`.\n",
    "\n",
    "```bash\n",
    "cd ~/master_ws/src/usafabot/usafabot_curriculum/my_scripts\n",
    "git clone git@github.com:ECE495/HOG_Demo.git\n",
    "cd HOG_Demo\n",
    "```\n",
    "\n",
    "Take a look at what is contained within the repo.  Essentially you have both a training data folder and a test folder.  We will now use a tool called **imglab** to annotate the images for building our detector.\n",
    "\n",
    "Browse to the [imglab tool](https://imglab.in/) and select **\"UMM, MAYBE NEXT TIME!\"**.\n",
    "\n",
    "In the bottom left of the site, select the load button and browse to the training folder:\n",
    "\n",
    "<img src=\"Figures/load.png\" width=\"200\" height=\"195.89\">\n",
    "\n",
    "Select the first stop sign and the **\"Rectangle\"** tool. \n",
    "\n",
    "<img src=\"Figures/rectangle.png\" width=\"100\" height=\"195.89\">\n",
    "\n",
    "Highlight the border of the stop sign: drag-and-draw a bounding rectangle, ensuring to **only** select the stop sign and to select all examples of the object in the image.\n",
    "\n",
    "> 📝️ **NOTE:** It’s important to label all examples of objects in an image; otherwise, Dlib will implicitly assume that regions not labeled are regions that should not be detected (i.e., hard-negative mining applied during extraction time).\n",
    "\n",
    "You can select a bounding box and hit the delete key to remove it.\n",
    "\n",
    "If you press `alt+left/right arrow` you can navigate through images in the slider and repeat highlighting the objects.\n",
    "\n",
    "Once all stop signs are complete hit `ctrl+e` to save the annotations (bounding box information) as a **\"Dlib XML\"** file within the `training` folder using a descriptive name such as `sl_annotations.xml`.\n",
    "\n",
    "<img src=\"Figures/xml.png\" width=\"300\" height=\"195.89\">\n",
    "\n",
    "We now need to create the code to build the detector based on our annotated training data.\n",
    "\n",
    "```bash\n",
    "cd ~/master_ws/src/usafabot/usafabot_curriculum/my_scripts/HOG_Demo\n",
    "touch trainDetector.py\n",
    "```\n",
    "\n",
    "Now open this in your favorite editor to add the following code.  I have built into the code the ability to provide command line arguments.  This will make the code a bit more flexible such that you don't need to recreate it in the future if you want to reuse if for another project.  You will provide two arguments at runtime.  First you need to tell the program where the .xml file is.  Second, you will state where you want to put the detector that you create... the detector should have a .svm extension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235ae6c",
   "metadata": {},
   "source": [
    "```python\n",
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import dlib\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-x\", \"--xml\", required=True, help=\"path to input XML file\")\n",
    "ap.add_argument(\"-d\", \"--detector\", required=True, help=\"path to output director\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# grab the default training options for the HOG + Linear SVM detector, then\n",
    "# train the detector -- in practice, the `C` parameter can be adjusted...\n",
    "# feel free to research and see if you can improve\n",
    "print(\"[INFO] training detector...\")\n",
    "options = dlib.simple_object_detector_training_options()\n",
    "options.C = 1.0\n",
    "options.num_threads = 4\n",
    "options.be_verbose = True\n",
    "dlib.train_simple_object_detector(args[\"xml\"], args[\"detector\"], options)\n",
    "\n",
    "# show the training accuracy\n",
    "print(\"[INFO] training accuracy: {}\".format(\n",
    "\tdlib.test_simple_object_detector(args[\"xml\"], args[\"detector\"])))\n",
    "\t\n",
    "# load the detector and visualize the HOG filter\n",
    "detector = dlib.simple_object_detector(args[\"detector\"])\n",
    "win = dlib.image_window()\n",
    "win.set_image(detector)\n",
    "dlib.hit_enter_to_continue()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e9af2",
   "metadata": {},
   "source": [
    "Once you have the code entered, you can test it with the following command.  Remember, you need to provide two command line arguments:\n",
    "\n",
    "```bash\n",
    "cd ~/master_ws/src/usafabot/usafabot_curriculum/my_scripts/HOG_Demo\n",
    "python3 trainDetector.py --xml training/sl_annotations.xml --detector training/sl_detector.svm\n",
    "```\n",
    "\n",
    "You may get a few errors pop up during execution based on your choice for bounding boxes.  Make sure you address those errors before continuing.  If everything executed correctly, you should ultimately see a picture of the HOG feature you designed.  \n",
    "\n",
    "Now it is time to build our code to test the detector.  The following code will make use of the imutils library as well.  Use pip3 to install it.\n",
    "\n",
    "You may get a few errors pop up during execution based on your choice for bounding boxes.  Make sure you address those errors before continuing.  If everything executed correctly, you should ultimately see a picture of the HOG feature you designed.  \n",
    "\n",
    "Now it is time to build our code to test the detector.\n",
    "\n",
    "```bash\n",
    "cd ~/master_ws/src/usafabot/usafabot_curriculum/my_scripts/HOG_Demo\n",
    "touch testDetector.py\n",
    "```\n",
    "\n",
    "Again, use your preferred editor to enter the code below: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24af3b5f",
   "metadata": {},
   "source": [
    "```python\n",
    "# import the necessary packages\n",
    "from imutils import paths\n",
    "import argparse\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--detector\", required=True, help=\"Path to trained object detector\")\n",
    "ap.add_argument(\"-t\", \"--testing\", required=True, help=\"Path to directory of testing images\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# load the detector\n",
    "detector = dlib.simple_object_detector(args[\"detector\"])\n",
    "\n",
    "# loop over the testing images\n",
    "for testingPath in paths.list_images(args[\"testing\"]):\n",
    "\t# load the image and make predictions\n",
    "\timage = cv2.imread(testingPath)\n",
    "\tboxes = detector(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\t\n",
    "\t# loop over the bounding boxes and draw them\n",
    "\tfor b in boxes:\n",
    "\t\t(x, y, w, h) = (b.left(), b.top(), b.right(), b.bottom())\n",
    "\t\tcv2.rectangle(image, (x, y), (w, h), (0, 255, 0), 2)\n",
    "\t\t\n",
    "\t# show the image\n",
    "\tcv2.imshow(\"Image\", image)\n",
    "\tcv2.waitKey(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fddc1b",
   "metadata": {},
   "source": [
    "Run the test detector:\n",
    "\n",
    "```bash\n",
    "cd ~/master_ws/src/usafabot/usafabot_curriculum/my_scripts/HOG_Demo\n",
    "python3 testDetector.py --detector training/sl_detector.svm --testing test\n",
    "```\n",
    "\n",
    "OK, so how did you do? What surprises did you have? What might you consider to improve the detector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439ea22",
   "metadata": {},
   "source": [
    "### Summary\n",
    "You have now trained and tested your first detector! In the future you will train a new detector using the camera on your robot and a real stop sign. This will be used in your final project to detect and react to stop signs in the wild!\n",
    "\n",
    "### Assignment\n",
    "Research Dlib's simple object detector, and see how you might want to tune the options to improve the performance.\n",
    "\n",
    "### Cleanup\n",
    "In the Jupyter Notebook at the top menu bar select \"Kernel\" and \"Restart & Clear Output\". Shutdown the notebook server by typing `ctrl+c` within the terminal you ran `jupyter-notebook` in. Select 'y'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3452f",
   "metadata": {},
   "source": [
    "## Fiducial Markers\n",
    "\n",
    "In this lesson we will learn how fiducial markers are used in image processing. Specifically, we will utilize ROS tools to identify different [April Tags](https://april.eecs.umich.edu/papers/details.php?name=olson2011tags) and use the 3D position and orientation to determine the robot's distance from an object.\n",
    "\n",
    "A fiducial marker is an artificial feature used in creating controllable experiments, ground truthing, and in simplifying the development of systems where perception is not the central objective. A few examples of fiducial markers include ArUco Markers, AprilTags, and QR codes. Each of these different tags hold information such as an ID or, in the case of QR codes, websites, messages, and etc. We will primarily be focusing on AprilTags as there is a very robust ROS package already built.\n",
    "\n",
    "### AprilTag ROS\n",
    "Browse to the AprilTag_ROS package on the robot and edit the config file:\n",
    "\n",
    "```bash\n",
    "ssh pi@robotX\n",
    "roscd apriltag_ros/config\n",
    "sudo nano tags.yaml\n",
    "```\n",
    "\n",
    "This is where you provide the package with information about the tags it should identify. You should have gotten tags 0-3. Each of these tags is $.165 m$ wide and should have a corresponding name: \"tag_0\" (in the final project, you might want to change these names as we will be providing you commands that correspond to each tag). In the `tags.yaml` file, add a line for each tag under \"standalone tags\" (replace ... with last two tags):\n",
    "\n",
    "```python\n",
    "standalone_tags:\n",
    "  [\n",
    "  \t{id: 0, size: .165, name: tag_0},\n",
    "  \t{id: 1, size: .165, name: tag_1},\n",
    "  \t...\n",
    "  ]\n",
    "```\n",
    "\n",
    "Browse to your `lab4` launch folder on the robot and copy the `stop_detector.launch` to a new launch file named `apriltag.launch`.\n",
    "\n",
    "Edit the `apriltag.launch` file and comment out the `stop_detector` node and call the launch file for AprilTag_ROS Continuous Detection setting the required arguments to our topic names provided by the `usb_cam` node:\n",
    "\n",
    "```xml\n",
    "<include file=\"$(find apriltag_ros)/launch/continous_detection.launch\">\n",
    "\t<arg name=\"camera_name\" value=\"usb_cam\"/>\n",
    "\t<arg name=\"camera_frame\" value=\"usb_cam\"/>\n",
    "\t<arg name=\"image_topic\" value=\"image_raw\"/>\n",
    "</include>\n",
    "```\n",
    "\n",
    "Save and exit.\n",
    "\n",
    "Launch the `apriltag.launch` file. \n",
    "\n",
    "In a terminal on the master open the `rqt_image_view` node (`rosrun rqt_image_view rqt_image_view`) and select the `tag_detections_image` topic. If you hold up each tag, you should see a yellow box highlight the tag with an id in the middle of the tag.\n",
    "\n",
    "In another terminal on the master echo the topic `tag_detections`. What information do you see? Will the apriltag_ros node identify only one tag at a time? Which value do you think we would use to determine distance from the tag? What kind of message is this? What package does this message come from?\n",
    "\n",
    "### Printing tag information\n",
    "\n",
    "Create a node on the master in lab4 called `apriltag_dist.py`. Import the appropriate AprilTag message. Subscribe to the `tag_detections` topic. Print the identified AprilTag ID and distance. If the camera sees multiple tags, it should print the information for each tag.\n",
    "\n",
    "In your callback function you will want to create a for loop such as:\n",
    "\n",
    "```python\n",
    "for tag in data.detections:\n",
    "```\n",
    "\n",
    "Use print statements to determine the characteristics of the message (you can also google the message)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7ce25",
   "metadata": {},
   "source": [
    "### Checkpoint\n",
    "Show an instructor the printed tag information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "339px",
    "width": "355px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "312.867px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
